--- 
layout: post
title: Machine Learning Ex4 - Logistic Regression
categories: 
  - machinelearning
  - R
intro: "<a href=\"http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex4/ex4.html\">Exercise 4</a> is all about implementing Logistic Regression using Newton's Method, on a classification problem.<img src='http://al3xandr3.github.com/img/ml-ex4-fit.png' alt='ml-ex4-fit.png' />"
---

<script type="text/javascript" src="http://www.mathjax.org/mathjax/MathJax.js">
    MathJax.Hub.Config({
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "left",
        displayIndent: "2em",
 
        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
</script>

[Exercise 4][1] is all about implementing Logistic Regression using Newton's
Method, on a classification problem.

For all this to make sense i suggest having a look at [Andrew Ng machine
learning lectures][2] on [openclassroom][2].

We start with a dataset representing 40 students who were admitted to college
and 40 students who were not admitted, and their corresponding grades for 2
exams. _Your mission, should you decide to accept it_ is to build a binary
classification model that estimates college admission chances based on a
student's scores on two exams(test1 and test2).

With implementation in R.

## Plot the Data

We start by looking at the data.

    
    google.spreadsheet <- function (key) {
      library(RCurl)
      # ssl validation off
      ssl.verifypeer <- FALSE
    
      tt <- getForm("https://spreadsheets.google.com/spreadsheet/pub", 
                    hl ="en_GB",
                    key = key, 
                    single = "true", gid ="0", 
                    output = "csv", 
                    .opts = list(followlocation = TRUE, verbose = TRUE)) 
    
      read.csv(textConnection(tt), header = TRUE)
    }
    
    # load the data
    mydata = google.spreadsheet("0AnypY27pPCJydC1vRVEzM1VJQnNneFo5dWNzR1F5Umc")
    
    # plots
    plot(mydata$test1[mydata$admitted == 0], mydata$test2[mydata$admitted == 0], xlab="test1", ylab="test2", , col="red")
    points(mydata$test1[mydata$admitted == 1], mydata$test2[mydata$admitted == 1], col="blue", pch=2)
    legend("bottomright", c("not admitted", "admitted"), pch=c(1, 2), col=c("red", "blue") )
    

![http://al3xandr3.github.com/img/ml-ex4-plotdata.png][3]

## A Bit of Theory

Most of the ideas explored in linear regression apply in same way, first we
define what the hypothesis equation looks like(the mathematical representation
of this knowledge). It originates from the line equation, but has now evolved
into a new equation that returns values between \[0,1\] suited for binary
classification. That is, we made up an equation that given test1 value and
test2 value, will return the probability that the student will be
admitted(y=1) into college:

<script type="math/tex; mode=display">
h_\theta(x) = g(\theta^T x) = \frac{1}{ 1 + e ^{- \theta^T x} }
</script>

g is the sigmoid function. And this returns:

<script type="math/tex; mode=display">
h_\theta(x) = P (y=1 | x; \theta)
</script>

Now we need to find the \(\theta\) parameters for getting a working hypothesis
equation. To help with that search we define a cost equation, that for a given
\(\theta\) returns how far off we are compared to the sample data.

<script type="math/tex; mode=display">
J(\theta) = \frac{1}{m} \sum_{i=1}^m ((-y)log(h_\theta(x)) - (1 - y) log(1- h_\theta(x)) )
</script>

The lower the cost the better(closer to real data we get). Thus, the goal
becomes to minimize the cost.

We can use [Newton's method][4] for that. Newton's method, similarly to
gradient descent, is a way to search for the 0(minimum) of the derivative of
the cost function. And after doing some math, the iterative \(\theta\) updates
using Newton's method is defined as:

<script type="math/tex; mode=display">
\theta^{(t+1)} = \theta^{(t)} -  H^{-1} \nabla_{\theta}J
</script>

And the gradient and Hessian are defined like so(in vectorized versions):

<script type="math/tex; mode=display">
\nabla_{\theta}J  = \frac{1}{m} \sum_{i=1}^m (h_\theta(x) - y) x
</script>

<script type="math/tex; mode=display">
H = \frac{1}{m} \sum_{i=1}^m [h_\theta(x) (1 - h_\theta(x)) x^T x]
</script>

## Implementation

First we implement the above equations:

    
    # sigmoid
    g = function (z) {
      return (1 / (1 + exp(-z) ))
    } # plot(g(c(1,2,3,4,5,6)))
    
    # hypothesis 
    h = function (x,th) {
      return( g(x %*% th) )
    } # h(x,th)
    
    # cost
    J = function (x,y,th,m) {
      return( 1/m * sum(-y * log(h(x,th)) - (1 - y) * log(1 - h(x,th))) )
    } # J(x,y,th,m)
    
    # derivative of J (gradient)
    grad = function (x,y,th,m) {
      return( 1/m * t(x) %*% (h(x,th) - y))
    } # grad(x,y,th,m)
    
    # Hessian
    H = function (x,y,th,m) {
      return (1/m * t(x) %*% x * diag(h(x,th)) * diag(1 - h(x,th)))
    } # H(x,y,th,m)
    

Make it go (iterate until convergence):

    
    # setup variables
    j = array(0,c(10,1))
    m = length(mydata$test1)
    x = matrix(c(rep(1,m), mydata$test1, mydata$test2), ncol=3)
    y = matrix(mydata$admitted, ncol=1)
    th = matrix(0,3)
    
    # iterate 
    # note that the newton's method converges fast, 10x is enough
    for (i in 1:10) {
      j[i] = J(x,y,th,m) # stores each iteration Cost
      th = th - solve(H(x,y,th,m)) %*% grad(x,y,th,m) 
    }
    

Have a look at the cost function by iteration:

    
    plot(j, xlab="iterations", ylab="cost J")
    

![http://al3xandr3.github.com/img/ml-ex4-j.png][5]

See that the number of iterations needed is only 4-5, converges much faster
than gradient descent.

Exercise questions:

    
    # 1. What values of  did you get? How many iterations were required for convergence?
    print("1.")
    print(th)
    
    # 2. What is the probability that a student with a score of 20 on Exam 1
    # and a score of 80 on Exam 2 will not be admitted?
    print("2.")
    print((1 - g(c(1, 20, 80) %*% th))* 100)
    
             
              1
    [1] "1."
                [,1]
    [1,] -16.4469479
    [2,]   0.1457278
    [3,]   0.1618285
    [1] "2."
             [,1]
    [1,] 64.24722
    

To visualize the fit, an important remark is that: \\(P(y=1 | x ;\\theta) =
0.5\\) that happens when:
<script type="math/tex; mode=display">
 \theta^T x=0
</script>
Thus



    
    # when ax0 + bx2 + cx3 = 0 is the middle(decision boundary line),
    # so given x1 from sample data, solving to x2, we get:
    x2 = (-1/th[3,]) * ((th[2,] * x1) + th[1,])
    
    # get 2 points (that will define a line)
    x1 = c(min(x[,2]), max(x[,2]))
    
    # plot
    plot(x1,x2, type='l',  xlab="test1", ylab="test2")
    points(mydata$test1[mydata$admitted == 0], mydata$test2[mydata$admitted == 0], col="red")
    points(mydata$test1[mydata$admitted == 1], mydata$test2[mydata$admitted == 1], col="blue", pch=2)
    legend("bottomright", c("not admitted", "admitted"), pch=c(1, 2), col=c("red", "blue") )
    

![http://al3xandr3.github.com/img/ml-ex4-fit.png][6]

Beautiful

#### Notes & References:

* Thanks Tal Galili for adding my blog into the [R-bloggers.com][7] list. [Go have a peek, R-bloggers][7] is a great source of R information. 
* [Exercise 4 here][1]
* [Lectures here][2]
* Thanks to Andrew Ng and [OpenClassRoom][8] for the great lessons. 

   [1]: http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex4/ex4.html
   [2]: http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning
   [3]: http://al3xandr3.github.com/img/ml-ex4-plotdata.png
   [4]: http://en.wikipedia.org/wiki/File:NewtonIteration_Ani.gif
   [5]: http://al3xandr3.github.com/img/ml-ex4-j.png
   [6]: http://al3xandr3.github.com/img/ml-ex4-fit.png
   [7]: http://www.r-bloggers.com/
   [8]: http://openclassroom.stanford.edu/MainFolder/HomePage.php

