--- 
layout: post
title: Machine Learning Ex5.1 - Regularized Linear Regression
categories: 
  - machinelearning
  - R
intro: "<a href=\"http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&amp;doc=exercises/ex5/ex5.html\">Exercise 5.1</a> Improves the Linear Regression implementation done in <a href=\"http://al3xandr3.github.com/2011/03/08/ml-ex3.html\">Exercise 3</a> by adding a regularization parameter that reduces the problem of over-fitting. <img src=\"http://al3xandr3.github.com/img/ml-ex51-regularized.png\" alt=\"ml-ex51-regularized.png\" />"
---


<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/1.1-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML-full">
    MathJax.Hub.Config({
            jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "left",
        displayIndent: "2em",
 
        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
</script>

[Exercise 5.1][1] Improves the Linear Regression implementation done in
[Exercise 3][2] by adding a regularization parameter that reduces the problem
of over-fitting.

Over-fitting occurs especially when fitting a high-order polynomial, that we
will try to do here.

With implementation in R.

## Data

Here's the points we will make a model from:

    
    google.spreadsheet <- function (key) {
      library(RCurl)
      # ssl validation off
      ssl.verifypeer <- FALSE
    
      tt <- getForm("https://spreadsheets.google.com/spreadsheet/pub", 
                    hl ="en_GB",
                    key = key, 
                    single = "true", gid ="0", 
                    output = "csv", 
                    .opts = list(followlocation = TRUE, verbose = TRUE)) 
    
      read.csv(textConnection(tt), header = TRUE)
    }
    
    # load the data
    mydata = google.spreadsheet("0AnypY27pPCJydGhtbUlZekVUQTc0dm5QaXp1YWpSY3c")
    
    # view data
    plot(mydata)
    

![http://al3xandr3.github.com/img/ml-ex51-data.png][3]

## Theory

We will fit a 5th order polynomial, so the hypothesis is:

<script type="math/tex; mode=display">
h_\theta(x) = \theta_0 x_0 + \theta_1 x_1 + \theta_2 x_2^2 + \theta_3 x_3^3 + \theta_4 x_4^4 + \theta_5 x_5^5
</script>

With x_0 = 1

The idea of the regularization is to blunt the fit a bit, i.e. loosen up the
tight fit.

For that we define the cost function like so:

<script type="math/tex; mode=display">
J(\theta) = \frac{1}{2m} [\sum_{i=1}^m ((h_\theta(x^{(i)}) - y^{(i)})^2) + \lambda \sum_{i=1}^n \theta^2]
</script>

The Lambda is called the regularization parameter.

The regularization parameter added at the end will influence the exact cost
values on all parameters. This will reflect in the search for the \(\theta\)
parameters and consequently loosen up the tight fit.

After some math that is not shown here, the **normal equations** with the
regularization parameter added, become:

<script type="math/tex; mode=display">
\theta = (X^T X + \lambda \begin{bmatrix} 0 & & & \\ & 1 & & \\ & & ... & \\ & & & 1 \end{bmatrix} )^{-1} (X^T y)
</script>

## Implementation

We will try 3 different lambda values to see how it influences the fit.
Starting with lambda=0 where we can see the fit without the
regularization parameter.

    
    # setup variables
    m = length(mydata$x) # samples
    x = matrix(c(rep(1,m), mydata$x, mydata$x^2, mydata$x^3, mydata$x^4, mydata$x^5), ncol=6)
    n = ncol(x) # features
    y = matrix(mydata$y, ncol=1)
    lambda = c(0,1,10)
    d = diag(1,n,n)
    d[1,1] = 0
    th = array(0,c(n,length(lambda)))
    
    # apply normal equations for each of the lambda's
    for (i in 1:length(lambda)) {
      th[,i] = solve(t(x) %*% x + (lambda[i] * d)) %*% (t(x) %*% y)
    }
    
    # plot
    plot(mydata)
    
    # lets create many points
    nwx = seq(-1, 1, len=50);
    x = matrix(c(rep(1,length(nwx)), nwx, nwx^2, nwx^3, nwx^4, nwx^5), ncol=6)
    lines(nwx, x %*% th[,1], col="blue", lty=2)
    lines(nwx, x %*% th[,2], col="red", lty=2)
    lines(nwx, x %*% th[,3], col="green3", lty=2)
    legend("topright", c(expression(lambda==0), expression(lambda==1),expression(lambda==10)), lty=2,col=c("blue", "red", "green3"), bty="n")
    

![http://al3xandr3.github.com/img/ml-ex51-regularized.png][4]

With the lambda=0 the fit is very tight to the original points (the blue
line) but as we increase lambda, the model gets less tight(more generalized)
and thus avoiding over-fitting.

### References:

- [Exercise 3, original Linear Regression implementation][2]
- Thanks to Andrew Ng and [OpenClassRoom][5] for the great lessons. 

   [1]: http://openclassroom.stanford.edu/MainFolder/DocumentPage.php?course=MachineLearning&doc=exercises/ex5/ex5.html
   [2]: http://al3xandr3.github.com/2011/03/08/ml-ex3.html
   [3]: http://al3xandr3.github.com/img/ml-ex51-data.png
   [4]: http://al3xandr3.github.com/img/ml-ex51-regularized.png
   [5]: http://openclassroom.stanford.edu/MainFolder/HomePage.php

